---
title: 'InterHand 2.6M'
date: 2021-06-17
permalink: /posts/2021/06/interhand26m/
tags:
  - 3d hand
  - gesture
---


# InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image

- 2.6M labeled single and interacting hand frames;
- stroing baseline for this new dataset.

## 简介

本数据集的提出主要是为了解决手势交互的问题.

<div align=center><img src="https://z3.ax1x.com/2021/06/07/2wlCWR.png" width=80% /></div>

数据集简要介绍:

- 数据集由标定过的多视角高分辨率相机;
- 使用一种半自动标记方法给数据打标签, i.e., 人工标注+自动标注.

几个问题:
- 如何自动标注的?
- 如何人工标注?
- 如何调整标注?
- 是否需要对手进行检测? bbox如何获得?
- 对手部距离鲁棒性如何?
- 数据集背景均为黑色, 在in-the-wild场景下表现如何?

强baseline InterNet简要介绍:

- 根据一张RGB图片获得交互或单独手部3D姿态估计;
- InterHand用来预测惯用手, 2.5D左右手姿态和双手的相对深度.
  - 惯用手可以表明左手或者右手是否在图片中, 以便在测试阶段时, InterHand能够排除并不存在的手;
  - 2.5D手部姿态由x和y轴的手部姿态和相对于z轴的root joint(手腕)深度, 为了将2.5D升到3D, 我们需要从RootNet中获得绝对深度.
  - 本文使用InterNet根据输入图片中交互手的外表来预测右手相对于左手的深度, 当左右手都在输入图片中时, 相对深度可代替RootNet的绝对深度.

实验表明对交互手进行姿态估计难度更大, 从而证明了该数据集提出的必要性.

## InterHand2.6M

### 数据集获取

- 80-140摄像机
- 30-90 fps
- 350-450个LED灯提供标准光照
- 原始分辨率: 4096x2668
- 26个subjects: 男19+女7
- 两种动作序列:
  - Peak Pose(PP): 默认姿态-->预定义姿态(比如握拳)-->默认姿态, 左右手各40个预定义动作序列+13个交互动作序列, 默认姿态为: 手部在胸前, 手指无接触, 手掌朝向侧面;
  - Range of Motion(ROM): 交流手势, 比如挥手. 左右手各15个交流手势+17个交互交流手势.

如何标注?

- 单手关节点21个, 双手42个. 对于每个手指来说, 标记了指尖和三个关节的旋转中心. 21个关节点: 20手指节点+1手腕节点.
- 六张图一起标注: 六张图为同一时刻不同角度的拍摄的图片, 标记人员标其中两张图, 剩下四张图的标注点用三角测量(triangulation)和重映射(re-projection), 这样标记保证了标注在3D空间的一致性.
- 标注分为两步:
  - 第一步: 人工标注. 
  - 第二步: 自动标注. 

公布的数据集格式:
- 两种格式: 512x334--5fps 和 512x334--30fps.
- 标注文件包括: 相机型号, subject索引, 相机索引, bbox, 惯用手, 相机参数, 3D关节点坐标.

## 网络结构

- Backbone: ResNet50
- 三个输出: 判断左手or右手, 2.5D左右手姿态估计, 左手相对于右手的深度.

### 判断左右手

输入为RenNet50的features $F$, 输出为二分类.

### 2.5D左右手姿态

3D Gaussian heatmaps.

Upsampler网络结构:
- 3 DeConv
- 1 Conv

输出维度: $\mathbb{R}^{J\times D\times H\times W}$.

### 左手相对于右手的深度

网络结构:
- 2 FC layers
- output: 1D heatmap
- soft-argmax

### 最终3D交互手势姿态输出

右手$P_{3D}^{R}$和左手$P_{3D}^{L}$的输出可表示如下:

$$P_{3D}^R=\Pi (T^{-1}P_{2.5D}^{R}+Z^R)$$ 

和

$$P_{3D}^L=\Pi (T^{-1}P_{2.5D}^{L}+Z^L)$$

其中, $\Pi$和$T^{-1}$分别为相机反投射矩阵和逆仿射矩阵(i.e. 2D 剪裁和resize等). $Z^R \in \mathbb{R}^{1\times 3}$和$Z^L \in \mathbb{R}^{1\times 3}$可表示如下:

$$Z^R=[(0),(0),(z^R)]$$

和

$$Z^L=\left\{\begin{matrix}
[(0),(0),(z^L)], & \mathrm{if}\;  h^R < 0.5 \\ 
[(0),(0),(z^R+z^{R\rightarrow L})], & \mathrm{otherwise}
\end{matrix}\right.$$

其中, $z^R$和$z^L$为右手和左手的根节点的绝对深度.

### Loss

- Handedness loss: 二分类
- 2.5D hand pose loss: 预测与GT的3D Gaussian Heatmap的L2 Loss. 如果某只手没出现, 则其对应的loss为0.
- 左手相对于右手的深度: L1 Loss. 当只有一只手时, 该loss为零.

## 实验

### 实验设置

- Backbone: ResNet50 pre-trained on ImageNet dataset.
- Adam
- batchSize: 64
- Input Size: $256\times 256$
- Heatmap Size: $64\times 64$
- Aug: translation, scaling, rotation, horizontal flip, color jittering.

### Protocol:

- 左右手分类是否准确?
- MPJPE: 单位mm, 欧氏距离, 手部关键节点预测是否准确?
- MRRPE: 单位mm, 欧氏距离, 相对深度是否准确?

### Evaluation performance original images

<div align=center><img src="https://z3.ax1x.com/2021/06/07/200y6O.png" width=80% /></div>